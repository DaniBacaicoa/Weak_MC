{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266ce908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.losses1 import MarginalChainProperLoss, ForwardProperLoss, scoring_matrix\n",
    "from src.dataset import Data_handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e733aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data_handling(dataset='mnist',train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d11ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__add__',\n",
       "  <bound method Dataset.__add__ of <src.dataset.Data_handling object at 0x0000021BF1E6FE00>>),\n",
       " ('__annotations__', {}),\n",
       " ('__class__', src.dataset.Data_handling),\n",
       " ('__class_getitem__', <function Data_handling.__class_getitem__>),\n",
       " ('__delattr__',\n",
       "  <method-wrapper '__delattr__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__dict__',\n",
       "  {'dataset': 'MNIST',\n",
       "   'dataset_source': None,\n",
       "   'tr_size': 0.8,\n",
       "   'val_size': None,\n",
       "   'test_size': None,\n",
       "   'weak_labels': None,\n",
       "   'virtual_labels': None,\n",
       "   'batch_size': 64,\n",
       "   'shuffle': False,\n",
       "   'splitting_seed': None,\n",
       "   'transform': Compose(\n",
       "       ToTensor()\n",
       "       Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "   ),\n",
       "   'train_dataset': Dataset MNIST\n",
       "       Number of datapoints: 60000\n",
       "       Root location: Datasets/raw_datasets\n",
       "       Split: Train\n",
       "       StandardTransform\n",
       "   Transform: Compose(\n",
       "                  ToTensor()\n",
       "                  Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "              ),\n",
       "   'test_dataset': Dataset MNIST\n",
       "       Number of datapoints: 10000\n",
       "       Root location: Datasets/raw_datasets\n",
       "       Split: Test\n",
       "       StandardTransform\n",
       "   Transform: Compose(\n",
       "                  ToTensor()\n",
       "                  Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "              ),\n",
       "   'num_classes': 10,\n",
       "   'train_num_samples': 60000,\n",
       "   'test_num_samples': 10000,\n",
       "   'num_features': 784}),\n",
       " ('__dir__', <function Data_handling.__dir__()>),\n",
       " ('__doc__', None),\n",
       " ('__eq__',\n",
       "  <method-wrapper '__eq__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__format__', <function Data_handling.__format__(format_spec, /)>),\n",
       " ('__ge__',\n",
       "  <method-wrapper '__ge__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__getattribute__',\n",
       "  <method-wrapper '__getattribute__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__getitem__',\n",
       "  <bound method Data_handling.__getitem__ of <src.dataset.Data_handling object at 0x0000021BF1E6FE00>>),\n",
       " ('__getstate__', <function Data_handling.__getstate__()>),\n",
       " ('__gt__',\n",
       "  <method-wrapper '__gt__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__hash__',\n",
       "  <method-wrapper '__hash__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__init__',\n",
       "  <bound method Data_handling.__init__ of <src.dataset.Data_handling object at 0x0000021BF1E6FE00>>),\n",
       " ('__init_subclass__', <function Data_handling.__init_subclass__>),\n",
       " ('__le__',\n",
       "  <method-wrapper '__le__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__lt__',\n",
       "  <method-wrapper '__lt__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__module__', 'src.dataset'),\n",
       " ('__ne__',\n",
       "  <method-wrapper '__ne__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__orig_bases__', (typing.Generic[+_T_co],)),\n",
       " ('__parameters__', ()),\n",
       " ('__reduce__', <function Data_handling.__reduce__()>),\n",
       " ('__reduce_ex__', <function Data_handling.__reduce_ex__(protocol, /)>),\n",
       " ('__repr__',\n",
       "  <method-wrapper '__repr__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__setattr__',\n",
       "  <method-wrapper '__setattr__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__sizeof__', <function Data_handling.__sizeof__()>),\n",
       " ('__str__',\n",
       "  <method-wrapper '__str__' of Data_handling object at 0x0000021BF1E6FE00>),\n",
       " ('__subclasshook__', <function Data_handling.__subclasshook__>),\n",
       " ('__weakref__', None),\n",
       " ('batch_size', 64),\n",
       " ('dataset', 'MNIST'),\n",
       " ('dataset_source', None),\n",
       " ('get_data',\n",
       "  <bound method Data_handling.get_data of <src.dataset.Data_handling object at 0x0000021BF1E6FE00>>),\n",
       " ('get_dataloader',\n",
       "  <bound method Data_handling.get_dataloader of <src.dataset.Data_handling object at 0x0000021BF1E6FE00>>),\n",
       " ('include_virtual',\n",
       "  <bound method Data_handling.include_virtual of <src.dataset.Data_handling object at 0x0000021BF1E6FE00>>),\n",
       " ('include_weak',\n",
       "  <bound method Data_handling.include_weak of <src.dataset.Data_handling object at 0x0000021BF1E6FE00>>),\n",
       " ('num_classes', 10),\n",
       " ('num_features', 784),\n",
       " ('shuffle', False),\n",
       " ('splitting_seed', None),\n",
       " ('test_dataset',\n",
       "  Dataset MNIST\n",
       "      Number of datapoints: 10000\n",
       "      Root location: Datasets/raw_datasets\n",
       "      Split: Test\n",
       "      StandardTransform\n",
       "  Transform: Compose(\n",
       "                 ToTensor()\n",
       "                 Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "             )),\n",
       " ('test_num_samples', 10000),\n",
       " ('test_size', None),\n",
       " ('tr_size', 0.8),\n",
       " ('train_dataset',\n",
       "  Dataset MNIST\n",
       "      Number of datapoints: 60000\n",
       "      Root location: Datasets/raw_datasets\n",
       "      Split: Train\n",
       "      StandardTransform\n",
       "  Transform: Compose(\n",
       "                 ToTensor()\n",
       "                 Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "             )),\n",
       " ('train_num_samples', 60000),\n",
       " ('transform',\n",
       "  Compose(\n",
       "      ToTensor()\n",
       "      Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "  )),\n",
       " ('val_size', None),\n",
       " ('virtual_labels', None),\n",
       " ('weak_labels', None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmembers(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1de0ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_size', 64),\n",
       " ('dataset', 'MNIST'),\n",
       " ('dataset_source', None),\n",
       " ('num_classes', 10),\n",
       " ('num_features', 784),\n",
       " ('shuffle', False),\n",
       " ('splitting_seed', None),\n",
       " ('test_dataset',\n",
       "  Dataset MNIST\n",
       "      Number of datapoints: 10000\n",
       "      Root location: Datasets/raw_datasets\n",
       "      Split: Test\n",
       "      StandardTransform\n",
       "  Transform: Compose(\n",
       "                 ToTensor()\n",
       "                 Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "             )),\n",
       " ('test_num_samples', 10000),\n",
       " ('test_size', None),\n",
       " ('tr_size', 0.8),\n",
       " ('train_dataset',\n",
       "  Dataset MNIST\n",
       "      Number of datapoints: 60000\n",
       "      Root location: Datasets/raw_datasets\n",
       "      Split: Train\n",
       "      StandardTransform\n",
       "  Transform: Compose(\n",
       "                 ToTensor()\n",
       "                 Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "             )),\n",
       " ('train_num_samples', 60000),\n",
       " ('transform',\n",
       "  Compose(\n",
       "      ToTensor()\n",
       "      Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "  )),\n",
       " ('val_size', None),\n",
       " ('virtual_labels', None),\n",
       " ('weak_labels', None)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = inspect.getmembers(Data, lambda a:not(inspect.isroutine(a)))\n",
    "[a for a in attributes if not(a[0].startswith('__') and a[0].endswith('__'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abb51d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset',\n",
       " 'dataset_source',\n",
       " 'tr_size',\n",
       " 'val_size',\n",
       " 'test_size',\n",
       " 'weak_labels',\n",
       " 'virtual_labels',\n",
       " 'batch_size',\n",
       " 'shuffle',\n",
       " 'splitting_seed',\n",
       " 'transform',\n",
       " 'train_dataset',\n",
       " 'test_dataset',\n",
       " 'num_classes',\n",
       " 'train_num_samples',\n",
       " 'test_num_samples',\n",
       " 'num_features']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in Data.__dict__.keys() if i[:1] != '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67de5e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: \n",
      " torch.Size([60000, 784])\n",
      "First training data sample: \n",
      " tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,  18.,  18.,\n",
      "        126., 136., 175.,  26., 166., 255., 247., 127.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94., 154.,\n",
      "        170., 253., 253., 253., 253., 253., 225., 172., 253., 242., 195.,  64.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49.,\n",
      "        238., 253., 253., 253., 253., 253., 253., 253., 253., 251.,  93.,  82.,\n",
      "         82.,  56.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,  18., 219., 253., 253., 253., 253., 253., 198., 182.,\n",
      "        247., 241.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
      "        253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0., 139., 253., 190.,   2.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11.,\n",
      "        190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240., 253.,\n",
      "        253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0., 249., 253., 249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253., 250., 182.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,  24., 114., 221., 253., 253., 253.,\n",
      "        253., 201.,  78.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213., 253.,\n",
      "        253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 171.,\n",
      "        219., 253., 253., 253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         55., 172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,  16.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.])\n",
      "Size of training targets: \n",
      " torch.Size([60000, 10])\n",
      "First training target: \n",
      " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training data: \\n\", Data.train_dataset.data.shape)\n",
    "print(\"First training data sample: \\n\", Data.train_dataset.data[0])\n",
    "print(\"Size of training targets: \\n\", Data.train_dataset.targets.shape)\n",
    "print(\"First training target: \\n\", Data.train_dataset.targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ecb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2ddd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4f9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
